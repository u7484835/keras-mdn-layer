{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Kanji with a time distributed MDN-RNN \n",
    "\n",
    "- This notebook is the same as the Kanji MDN-RNN except that it trains on predictions made over the whole sequence length.\n",
    "- The MDN-RNN is also written in Keras' functional API for good measure!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from context import * # imports the MDN layer \n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "%matplotlib inline\n",
    "\n",
    "# Only for GPU use:\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train from David Ha's Kanji dataset from Sketch-RNN: https://github.com/hardmaru/sketch-rnn-datasets\n",
    "# Other datasets in \"Sketch 3\" format should also work.\n",
    "import urllib.request\n",
    "url = 'https://github.com/hardmaru/sketch-rnn-datasets/raw/master/kanji/kanji.rdp25.npz'  \n",
    "urllib.request.urlretrieve(url, './kanji.rdp25.npz')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset:\n",
    "\n",
    "Includes about 11000 handwritten kanji characters divied into training, validation, and testing sets.\n",
    "\n",
    "For creative purposes, we may not need the validation or testing sets, and can just focus on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('./kanji.rdp25.npz', allow_pickle=True) as data:\n",
    "    train_set = data['train']\n",
    "    valid_set = data['valid']\n",
    "    test_set = data['test']\n",
    "    \n",
    "print(\"Training kanji:\", len(train_set))\n",
    "print(\"Validation kanji:\", len(valid_set))\n",
    "print(\"Testing kanji:\", len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup an MDN RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyperparameters:\n",
    "SEQ_LEN = 50\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_UNITS = 256\n",
    "EPOCHS = 100\n",
    "SEED = 2345  # set random seed for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "OUTPUT_DIMENSION = 3\n",
    "NUMBER_MIXTURES = 10\n",
    "\n",
    "inputs = keras.layers.Input(shape=(SEQ_LEN,OUTPUT_DIMENSION), name='inputs')\n",
    "lstm1_out = keras.layers.LSTM(HIDDEN_UNITS, name='lstm1', return_sequences=True)(inputs)\n",
    "lstm2_out = keras.layers.LSTM(HIDDEN_UNITS, name='lstm2', return_sequences=True)(lstm1_out)\n",
    "mdn_out = keras.layers.TimeDistributed(mdn.MDN(OUTPUT_DIMENSION, NUMBER_MIXTURES, name='mdn_outputs'), name='td_mdn')(lstm2_out)\n",
    "\n",
    "model = keras.models.Model(inputs=inputs, outputs=mdn_out)\n",
    "model.compile(loss=mdn.get_mixture_loss_func(OUTPUT_DIMENSION,NUMBER_MIXTURES), optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the Data and Train the Model\n",
    "\n",
    "- Chop up the data into slices of the correct length, generate `X` and `y` for the training process.\n",
    "- Very similar process to the previous RNN examples!\n",
    "- We end up with 330000 examples - a pretty healthy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for slicing up data\n",
    "def slice_sequence_examples(sequence, num_steps):\n",
    "    xs = []\n",
    "    for i in range(len(sequence) - num_steps - 1):\n",
    "        example = sequence[i: i + num_steps]\n",
    "        xs.append(example)\n",
    "    return xs\n",
    "\n",
    "def seq_to_overlapping_format(examples):\n",
    "    \"\"\"Takes sequences of seq_len+1 and returns overlapping\n",
    "    sequences of seq_len.\"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for ex in examples:\n",
    "        xs.append(ex[:-1])\n",
    "        ys.append(ex[1:])\n",
    "    return (xs,ys)\n",
    "\n",
    "# Prepare training data as X and Y.\n",
    "slices = []\n",
    "for seq in train_set:\n",
    "    slices +=  slice_sequence_examples(seq, SEQ_LEN+1)\n",
    "X, y = seq_to_overlapping_format(slices)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"Number of training examples:\")\n",
    "print(\"X:\", X.shape)\n",
    "print(\"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare validation data as X and Y.\n",
    "slices = []\n",
    "for seq in valid_set:\n",
    "    slices +=  slice_sequence_examples(seq, SEQ_LEN+1)\n",
    "Xval, yval = seq_to_overlapping_format(slices)\n",
    "\n",
    "Xval = np.array(Xval)\n",
    "yval = np.array(yval)\n",
    "\n",
    "print(\"Number of training examples:\")\n",
    "print(\"X:\", Xval.shape)\n",
    "print(\"y:\", yval.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the training!\n",
    "\n",
    "- We're not going to train in the tutorial!\n",
    "- These settings take about 220 seconds per epoch, about 6 hours for the whole training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "filepath=\"kanji_mdnrnn-{epoch:02d}.h5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, save_weights_only=True, verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "callbacks = [keras.callbacks.TerminateOnNaN(), checkpoint, early_stopping]\n",
    "history = model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=callbacks, validation_data=(Xval,yval))\n",
    "model.save('kanji_mdnrnn_model_time_distributed.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title(\"Kanji MDRNN Training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out the model! Generate some Kanji!\n",
    "\n",
    "We need to create a decoding model with batch size 1 and sequence length 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding Model\n",
    "# Same as training model except for dimension and mixtures.\n",
    "\n",
    "decoder = keras.Sequential()\n",
    "decoder.add(keras.layers.LSTM(HIDDEN_UNITS, batch_input_shape=(1,1,OUTPUT_DIMENSION), return_sequences=True, stateful=True))\n",
    "decoder.add(keras.layers.LSTM(HIDDEN_UNITS, stateful=True))\n",
    "decoder.add(mdn.MDN(OUTPUT_DIMENSION, NUMBER_MIXTURES))\n",
    "decoder.compile(loss=mdn.get_mixture_loss_func(OUTPUT_DIMENSION,NUMBER_MIXTURES), optimizer=keras.optimizers.Adam())\n",
    "decoder.summary()\n",
    "\n",
    "#decoder.load_weights('kanji_mdnrnn_model_time_distributed.h5') # load weights independently from file\n",
    "#decoder.load_weights('kanji_mdnrnn-99.hdf5')\n",
    "decoder.load_weights('kanji_mdnrnn_model_time_distributed.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating drawings\n",
    "\n",
    "- First need some helper functions to view the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def zero_start_position():\n",
    "    \"\"\"A zeroed out start position with pen down\"\"\"\n",
    "    out = np.zeros((1, 1, 3), dtype=np.float32)\n",
    "    out[0, 0, 2] = 1 # set pen down.\n",
    "    return out\n",
    "\n",
    "def generate_sketch(model, start_pos, num_points=100):\n",
    "     return None\n",
    "\n",
    "def cutoff_stroke(x):\n",
    "    return np.greater(x,0.5) * 1.0\n",
    "\n",
    "def plot_sketch(sketch_array):\n",
    "    \"\"\"Plot a sketch quickly to see what it looks like.\"\"\"\n",
    "    sketch_df = pd.DataFrame({'x':sketch_array.T[0],'y':sketch_array.T[1],'z':sketch_array.T[2]})\n",
    "    sketch_df.x = sketch_df.x.cumsum()\n",
    "    sketch_df.y = -1 * sketch_df.y.cumsum()\n",
    "    # Do the plot\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    #ax1.scatter(sketch_df.x,sketch_df.y,marker='o', c='r', alpha=1.0)\n",
    "    # Need to do something with sketch_df.z\n",
    "    ax1.plot(sketch_df.x,sketch_df.y,'r-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVG Drawing Function\n",
    "\n",
    "Here's Hardmaru's Drawing Functions from _write-rnn-tensorflow_. Big hat tip to Hardmaru for this!\n",
    "\n",
    "Here's the source: https://github.com/hardmaru/write-rnn-tensorflow/blob/master/utils.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardmaru's Drawing Functions from write-rnn-tensorflow\n",
    "# Big hat tip\n",
    "# Here's the source:\n",
    "# https://github.com/hardmaru/write-rnn-tensorflow/blob/master/utils.py\n",
    "\n",
    "import svgwrite\n",
    "from IPython.display import SVG, display\n",
    "\n",
    "def get_bounds(data, factor):\n",
    "    min_x = 0\n",
    "    max_x = 0\n",
    "    min_y = 0\n",
    "    max_y = 0\n",
    "\n",
    "    abs_x = 0\n",
    "    abs_y = 0\n",
    "    for i in range(len(data)):\n",
    "        x = float(data[i, 0]) / factor\n",
    "        y = float(data[i, 1]) / factor\n",
    "        abs_x += x\n",
    "        abs_y += y\n",
    "        min_x = min(min_x, abs_x)\n",
    "        min_y = min(min_y, abs_y)\n",
    "        max_x = max(max_x, abs_x)\n",
    "        max_y = max(max_y, abs_y)\n",
    "\n",
    "    return (min_x, max_x, min_y, max_y)\n",
    "\n",
    "def draw_strokes(data, factor=1, svg_filename='sample.svg'):\n",
    "    min_x, max_x, min_y, max_y = get_bounds(data, factor)\n",
    "    dims = (50 + max_x - min_x, 50 + max_y - min_y)\n",
    "\n",
    "    dwg = svgwrite.Drawing(svg_filename, size=dims)\n",
    "    dwg.add(dwg.rect(insert=(0, 0), size=dims, fill='white'))\n",
    "\n",
    "    lift_pen = 1\n",
    "\n",
    "    abs_x = 25 - min_x\n",
    "    abs_y = 25 - min_y\n",
    "    p = \"M%s,%s \" % (abs_x, abs_y)\n",
    "\n",
    "    command = \"m\"\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if (lift_pen == 1):\n",
    "            command = \"m\"\n",
    "        elif (command != \"l\"):\n",
    "            command = \"l\"\n",
    "        else:\n",
    "            command = \"\"\n",
    "        x = float(data[i, 0]) / factor\n",
    "        y = float(data[i, 1]) / factor\n",
    "        lift_pen = data[i, 2]\n",
    "        p += command + str(x) + \",\" + str(y) + \" \"\n",
    "\n",
    "    the_color = \"black\"\n",
    "    stroke_width = 2\n",
    "\n",
    "    dwg.add(dwg.path(p).stroke(the_color, stroke_width).fill(\"none\"))\n",
    "\n",
    "    dwg.save()\n",
    "    display(SVG(dwg.tostring()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict a character and plot the result.\n",
    "temperature = 1.5 # seems to work well with rather high temperature (2.5)\n",
    "sigma_temp = 0.01\n",
    "\n",
    "p = zero_start_position()\n",
    "sketch = [p.reshape(3,)]\n",
    "\n",
    "for i in range(100):\n",
    "    params = decoder.predict(p.reshape(1,1,3))\n",
    "    p = mdn.sample_from_output(params[0], OUTPUT_DIMENSION, NUMBER_MIXTURES, temp=temperature, sigma_temp=sigma_temp)\n",
    "    sketch.append(p.reshape((3,)))\n",
    "\n",
    "sketch = np.array(sketch)\n",
    "decoder.reset_states()\n",
    "\n",
    "sketch.T[2] = cutoff_stroke(sketch.T[2])\n",
    "draw_strokes(sketch, factor=0.5)\n",
    "#plot_sketch(sketch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
